{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coursera - IBM Data Science Capstone Project ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brian Lee ###\n",
    "BrianTrewLee@gmail.com<br />\n",
    "LinkedIn: https://www.linkedin.com/in/brian-lee-64a2022b/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I were to open a Starbucks, where should I locate it? Clearly, the location is one of the most important business decisions for this venture. Is there a science to locating a StarBucks? Can we create a machine learning model to predict where a StarBucks is likely to be located? I'd like to find out.\n",
    "\n",
    "If a reliable model can be made,it could be used in the process of opening a store. It could be used as a final sanity check, or at the beginning stage, to select a promising location. If a trustworthy model predicts with a high degree of confidence that a Starbucks should be located in an area, but there is not one there yet, perhaps that is an opportunity.\n",
    "\n",
    "Any person interested in opening a Starbucks should be interested in these results. This includes myself, other potential franchisees, and the Starbucks corporation itself, which operates many of it's own stores. The Starbucks competition might also be interested as they could possibly gain competitive insigths. I also believe others might be interested in this procedure, as it might be applied to predicting the location of other entities, based on the same sort of data.\n",
    "\n",
    "The main purpose of this project is to prove the concept of predicting Starbuck's locations in general. I suspect it may work better or worse depending on the locations chosen for training and prediction, the radius size, and the specific features used in the model. I may vary those factors in order to prove the concept, which could then be applied carefully to a particular geographic area of interest at a later date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foursquare (https://foursquare.com) is a company that crowdsources location data, tying latitude-longitude coordinates (and other things) with public venues, including many businesses, such as Starbucks locations. The features for the Starbucks location classifier will come from venue data from the Fourquare API. I will use venue category names (such as 'bar', 'Chinese restaurant', 'coffee shop') as features to classify an area as either an area with a Starbucks, or an area without one. I will use the venue name (e.g. 'Starbucks') to determine whether or not the venue is a Starbucks. An area containing a Starbucks contains at least one venue named 'Starbucks' within the radius supplied to the Foursquare API's explore' endpoint.\n",
    "\n",
    "An area will be a circular area with a given radius. I have not determined that radius yet, and I might use more than one radius in this project. Within that radius, the Foursquare API will return venues. The larger the radius, the more venues that could be anticipated, and the higher likelihood of a Starbucks, but a larger radius is also less useful for business use. I have done some exploratory work with a radius of 300m.\n",
    "\n",
    "The areas to be chosen should contain a mix of areas with Starbuck's and those without. I have done some exploratory work in a particular area of San Francisco. I located Starbucks with Google Maps and got latitude and longitude points making a box around that area. I could choose the train and test points at random within an area, or I could generate a grid covering an entire area. Given the constraints my Foursquare Developer account places on my searches, and knowing that binary classifiers work better with a good number of examples of both classes, I may want to generate areas that I know will be heavy with Starbucks locations. To that end, I discovered a Kaggle dataset (https://www.kaggle.com/starbucks/store-locations) containing the latitude and longitude coordinates of 25,600 worldwide Starbucks locations. I may or may not use that resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Methodology ###\n",
    "\n",
    "I will generate a corpus of geographic points, and using a reasonable radius (perhaps 300m) a corpus of geographic areas. Each geographic area will have a feature set based on json data returned by the Foursquare API's 'explore' endpoint. I plan to use the category names of the returned venues as features for that area. I will use the venue names to determine whether oir not a Starbucks is in that area, and generate the labels from that. \n",
    "\n",
    "Once I have labelled data, I will use sklearn.model_selection.train_test_split to generate tarining and testing sets. Then I  can use any of a host of sklearn classifiers to generate predictive models. I have done some exploratory work with a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",

   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import urllib.request # open and read URLs\n",
    "\n",
    "import json # handle JSON files\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "\n",
    "import requests # handle requests\n",
    "import pandas as pd # process data as dataframes with Pandas\n",
    "import numpy as np # handle data in a vectorized manner with NumPy\n",
    "\n",
    "# !conda install -c conda-forge geopy --yes # uncomment this line if you haven't installed the GeoPy geocoding library yet\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "\n",
    "##!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't installed the Folium library yet\n",
    "import folium # map rendering library\n",
    "\n",
    "# Matplolib plotting library and associated modules\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "from sklearn.cluster import KMeans # for K-Means clustering with Scikit-Learn\n",
    "\n",
    "print(\"Libraries imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code was removed by Watson Studio for sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to gather recommended venues, with specifically the name and category, using the explore API\n",
    "\n",
    "def getNearbyVenues(latitudes, longitudes, radius=300, limit=100):\n",
    "#def getNearbyVenues(latitudes, longitudes, radius=300, limit=100):\n",
    "#def getNearbyVenues(latitudes, longitudes, radius=750, limit=100):\n",
    "#def getNearbyVenues(latitudes, longitudes, radius=2000, limit=100):\n",
    "    \n",
    "    count = 0\n",
    "    venues_list = []\n",
    "    for lat, lng in zip(latitudes, longitudes):\n",
    "        #create a unique name for this point that is easier todeal with than a combination of lat and lon\n",
    "        point_name = \"point\"+str(count)\n",
    "        print(point_name,':',lat,'-',lng)\n",
    "         \n",
    "\n",
    "            \n",
    "        # create the API request URL\n",
    "        url = \"https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}\".format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            limit)\n",
    "            \n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"][\"groups\"][0][\"items\"]\n",
    "        #print(\"resultscols=\",results.columns)\n",
    "        \n",
    "\n",
    "        \n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            point_name,\n",
    "            lat, \n",
    "            lng, \n",
    "            v[\"venue\"][\"name\"], \n",
    "            v[\"venue\"][\"location\"][\"lat\"], \n",
    "            v[\"venue\"][\"location\"][\"lng\"],  \n",
    "            v[\"venue\"][\"location\"][\"distance\"], \n",
    "            v[\"venue\"][\"categories\"][0][\"name\"]) for v in results])\n",
    "        count = count + 1\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = [\"Point Name\",\n",
    "                  \"Neighborhood Latitude\", \n",
    "                  \"Neighborhood Longitude\", \n",
    "                  \"Venue\", \n",
    "                  \"Venue Latitude\", \n",
    "                  \"Venue Longitude\", \n",
    "                  \"Venue Distance\",           \n",
    "                  \"Venue Category\"]\n",
    "    \n",
    "    return(nearby_venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_lat=37.7749\n",
    "sf_lon=-122.4194\n",
    "ch_lat=41.8781\n",
    "ch_lon=-87.6298\n",
    "la_lat=34.0522\n",
    "la_lon=-118.2437\n",
    "starbucks_m_lat=37.744644\n",
    "starbucks_m_lon=-122.452731\n",
    "import random\n",
    "\n",
    "#37.780253, -122.509117\n",
    "#37.804203, -122.459264 - upper left of starbucks intensive space\n",
    "#37.767547, -122.409342 - lower right ofstarbucksintensive space\n",
    "llat=37.767547\n",
    "ulat=37.804203\n",
    "llon=-122.459264\n",
    "ulon=-122.409342\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#37.599350, -122.496830\n",
    "#37.807957, -122.390854\n",
    "#reset parameters to bigger area\n",
    "##llat = 37.599350\n",
    "#ulat = 37.807957\n",
    "##llon = -122.496830\n",
    "##ulon = -122.390854\n",
    "\n",
    "\n",
    "points = []\n",
    "radius = 100 #in meters\n",
    "current_lat = ulat\n",
    "current_lon = llon\n",
    "import geopy\n",
    "from geopy.distance import VincentyDistance\n",
    "d = radius/1000 #in kilometers\n",
    "#d=d*3\n",
    "b=90 #in degrees 90 = east,180=south,270=west\n",
    "# given: lat1, lon1, b = bearing in degrees, d = distance in kilometers\n",
    "\n",
    "def lon_pass(current_lat, current_lon, ulat, ulon):\n",
    "    while (current_lat <= ulat) and (current_lon <= ulon) and (current_lat >= llat) and (current_lon >= llon):\n",
    "        origin = geopy.Point(current_lat, current_lon)\n",
    "        destination = VincentyDistance(kilometers=d).destination(origin, b)\n",
    "\n",
    "        current_lat, current_lon = destination.latitude, destination.longitude\n",
    "        print(current_lat, current_lon)\n",
    "        points.append((current_lat, current_lon))\n",
    " \n",
    "\n",
    "while (current_lat <= ulat) and (current_lon <= ulon) and (current_lat >= llat) and (current_lon >= llon):\n",
    "    lon_pass(current_lat, current_lon, ulat, ulon)\n",
    "    print('new row of coordinates')  \n",
    "    current_lon=llon#move back to beginning of row   \n",
    "    origin = geopy.Point(current_lat, current_lon)\n",
    "    destination = VincentyDistance(kilometers=d).destination(origin, 180)#move south one step for next row\n",
    "    current_lat, current_lon = destination.latitude, destination.longitude\n",
    "\n",
    "map = folium.Map(location=[ulat, ulon], zoom_start=10)\n",
    "points = pd.DataFrame(points)\n",
    "\n",
    "# add markers to map\n",
    "for lat, lng in zip(points[0],points[1]):\n",
    "    #label = '{}, {}'.format(neighborhood, borough)\n",
    "    #label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=5,\n",
    "        #popup=label,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='#3186cc',\n",
    "        fill_opacity=0.7,\n",
    "        parse_html=False).add_to(map)  \n",
    "    \n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Longitude\"].head()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "venues = getNearbyVenues(latitudes = points[0],\n",
    "                              longitudes =  points[1]\n",
    "                               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues['Venue'].unique()\n",
    "venues['Venue'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues[350:400]\n",
    "\n",
    "\n",
    "# one hot encoding\n",
    "onehot = pd.get_dummies(venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "# add neighborhood column back to dataframe\n",
    "onehot['point_name'] = venues['Point Name'] \n",
    "onehot['point_lat'] = venues['Neighborhood Latitude'] \n",
    "onehot['point_lon'] = venues['Neighborhood Longitude'] \n",
    "onehot['isStarbucks'] = (venues['Venue'] == \"Starbucks\")\n",
    "# move neighborhood column to the first column\n",
    "fixed_columns = [onehot.columns[-3]] + [onehot.columns[-2]] + [onehot.columns[-1]] + list(onehot.columns[:-3])\n",
    "onehot = onehot[fixed_columns]\n",
    "\n",
    "onehot.isStarbucks = onehot.isStarbucks.astype(int)\n",
    "\n",
    "onehot.head()\n",
    "onehot['isStarbucks'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = onehot.groupby('point_name').mean().reset_index()\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_venues = 250\n",
    "\n",
    "for point in grouped['point_name']:\n",
    "    print(\"----\"+point+\"----\")\n",
    "    temp = grouped[grouped['point_name'] == point].T.reset_index()\n",
    "    temp.columns = ['venue','freq']\n",
    "    temp = temp.iloc[2:]\n",
    "    temp['freq'] = temp['freq'].astype(float)\n",
    "    temp = temp.round({'freq': 2})\n",
    "    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = grouped['isStarbucks'] > 0\n",
    "y = pd.DataFrame(y)\n",
    "y = y.isStarbucks.astype(int)\n",
    "print(y.value_counts())\n",
    "X = grouped\n",
    "print(X.shape)\n",
    "X .drop(columns=['point_name', 'point_lat', 'point_lon', 'isStarbucks'], inplace=True)\n",
    "print(X.columns)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# create training and testing vars\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdytrain=pd.DataFrame(y_train)\n",
    "pdytest=pd.DataFrame(y_test)\n",
    "\n",
    "pdytrain['isStarbucks'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdytest['isStarbucks'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk  \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#RF = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=0)  \n",
    "RF = RandomForestClassifier(n_estimators=100, max_depth=200, random_state=0)  \n",
    "RF.fit(X_train, y_train)  \n",
    "RF.predict(X_train)  \n",
    "RF.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ybogus = y_test-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test=RF.predict(X_test)  \n",
    "RF.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about AUROC?\n",
    "prob_y = RF.predict_proba(X)\n",
    "prob_y\n",
    "prob_y_train = RF.predict_proba(X_train)\n",
    "prob_y_test = RF.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prob_y = [p[1] for p in prob_y]\n",
    "#prob_y\n",
    "#print( roc_auc_score(y, prob_y) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "print(roc_auc_score(y_test, prob_y_test[:,1]))\n",
    "print(roc_auc_score(y_train, prob_y_train[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test, pred_test[:,1])\n",
    "\n",
    "plt.clf()\n",
    "plot.plot(fpr, tpr)\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = RF.estimators_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "# Export as dot file\n",
    "export_graphviz(estimator, out_file='tree.dot', \n",
    "                feature_names = X.columns,\n",
    "                class_names = 'has_starbucks',\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to png using system command (requires Graphviz)\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "# Display in jupyter notebook\n",
    "from IPython.display import Image\n",
    "Image(filename = 'tree.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
