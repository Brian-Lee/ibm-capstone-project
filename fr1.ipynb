{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A full report consisting of all of the following components (15 marks):<br /><br />\n",
    "Introduction where you discuss the business problem and who would be interested in this project.<br /><br />\n",
    "Data where you describe the data that will be used to solve the problem and the source of the data.<br /><br />\n",
    "Methodology section which represents the main component of the report where you discuss and describe any exploratory data analysis that you did, any inferential statistical testing that you performed, and what machine learnings were used and why.<br /><br />\n",
    "Results section where you discuss the results.<br /><br />\n",
    "Discussion section where you discuss any observations you noted and any recommendations you can make based on the results.<br /><br />\n",
    "Conclusion section where you conclude the report.<br /><br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coursera - IBM Data Science Capstone Project ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brian Lee ###\n",
    "BrianTrewLee@gmail.com<br />\n",
    "LinkedIn: https://www.linkedin.com/in/brian-lee-data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I were to open a Starbucks, where should I locate it? Clearly, the location is one of the most important business decisions for this venture. Is there a science to locating a StarBucks? Can we create a machine learning model to predict where a StarBucks is likely to be located? I'd like to find out.\n",
    "\n",
    "If a reliable model can be made, it could be used in the process of opening a store. It could be used as a final sanity check, or at the beginning stage, to select a promising location. If a trustworthy model predicts with a high degree of confidence that a Starbucks should be located in an area, but there is not one there yet, perhaps that is an opportunity.\n",
    "\n",
    "Any person interested in opening a Starbucks should be interested in these results. This includes myself, other potential franchisees, and the Starbucks corporation itself, which operates many of it's own stores. The Starbucks competition might also be interested as they could possibly gain competitive insights. I also believe others might be interested in this procedure, as it might be applied to predicting the location of other entities, based on the same sort of data.\n",
    "\n",
    "The main purpose of this project is to prove the concept of predicting Starbuck's locations in general. I suspect it may work better or worse depending on the locations chosen for training and prediction, the radius size, and the specific features used in the model. I may vary those factors in order to prove the concept, which could then be applied carefully to a particular geographic area of interest at a later date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foursquare (https://foursquare.com) is a company that crowdsources location data, tying latitude-longitude coordinates (and other things) with public venues, including many businesses, such as Starbucks locations. The features for the Starbucks location classifier will come from venue data from the Fourquare API. I will use venue category names (such as 'bar', 'Chinese restaurant', 'coffee shop') as features to classify an area as either an area with a Starbucks, or an area without one. I will use the venue name (e.g. 'Starbucks') to determine whether or not the venue is a Starbucks. An area containing a Starbucks contains at least one venue named 'Starbucks' within the radius supplied to the Foursquare API's explore' endpoint.\n",
    "\n",
    "An area will be a circular area with a given radius. I used a radius of 300m for this project. Within that radius, the Foursquare API will return venues. The larger the radius, the more venues that could be anticipated, and the higher likelihood of a Starbucks, but a larger radius is also less useful for business use.\n",
    "\n",
    "The quality of the Foursquare venue data is good, but not perfect. I noticed venues returned as \"SUBWAY\" separately from other venues returned as \"Subway.\" I cleaned that by altering the case of the venue names after received by the API. I also noticed that the crowdsourced latitude-longitude coordinates for Starbucks locations often include duplicative results, slightly offset. For instance, there could be a cluster of tightly packed coordinates returned as Starbucks locations, giving the impression of many Starbucks next to each other. I suspect this actually reflects different geographic coordinates representing a single Starbucks venue. Additionally, there are areas (for instance in my validation area in Illinois) where no venues are reported within the radius. In some cases, it can be seen that some venues (such as churchs) are in fact located nearby.\n",
    "\n",
    "The areas chosen for training/testing and validation data contain a mix of areas with Starbuck's and those without. I selected a geographic rectangle in San Francisco for training/testing. With a reasonably small radius, most points will have true labels of 0 (or not near a Starbucks.) In order to balance the classes for better binary classification performance, I added a large number of additional geographic points to the training set. These additional coordinates come from the Kaggle dataset (https://www.kaggle.com/starbucks/store-locations) containing the latitude and longitude coordinates of 25,600 worldwide Starbucks locations. The idea was that these added points should have true labels of 1 (near a Starbucks), and most do. However, I discovered that they do not all. To that end, the best balancing would be a ratio of about 1.5/1. However, I found that balancing 1:1 seemed sufficient for training performance. The fact that these added points don't all appear to have labels of 1 is distressing. Perhaps that is a sign that something is wrong, or perhaps it is due to different data sources not agreeing on exactly where a current Starbucks venue is located. In any event, my training data is comprised of a grid of points inside a rectangle, combined with an equal number of points throughout California. \n",
    "\n",
    "I wanted to validate the results on a completely separate area, with a naturally occurring distribution of classes. I chose a rectangular area in Illinois, near Chicago, with a decent number of starbucks locations according to Google Maps. When I got the venue category names I used to create onehot features, I discovered that I had a slightly different set of features. There were a few (3) venue category types that did not occur in my training/testing set, and a few dozen (~30) that occurred in the validation data that did not occur in the training/testing data. I could have retrained my model with a superset of features that wold satisfy both regimes, but I chose instead to reformat my validation features to fit my trained model. I added three dummy columns to the validation data, and threw out 30 columns, resulting in a feature set of 452. I felt this approach was a good approach to test the generalizability and durability of the trained model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "I have done some exploratory work in a particular area of San Francisco. I located Starbucks with Google Maps and got latitude and longitude points making a box around that area. I could choose the train and test points at random within an area, or I could generate a grid covering an entire area. Given the constraints my Foursquare Developer account places on my searches, and knowing that binary classifiers work better with a good number of examples of both classes, I may want to generate areas that I know will be heavy with Starbucks locations. To that end, I discovered a  I may or may not use that resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Methodology ###\n",
    "\n",
    "I will generate a corpus of geographic points, and using a reasonable radius (perhaps 300m) a corpus of geographic areas. Each geographic area will have a feature set based on json data returned by the Foursquare API's 'explore' endpoint. I plan to use the category names of the returned venues as features for that area. I will use the venue names to determine whether oir not a Starbucks is in that area, and generate the labels from that. \n",
    "\n",
    "Once I have labelled data, I will use sklearn.model_selection.train_test_split to generate tarining and testing sets. Then I  can use any of a host of sklearn classifiers to generate predictive models. I have done some exploratory work with a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "CLIENT_ID='0C4GFTHUCEID10D2EF5OUUVTPDE3LRKIKOCWYOXSVTN0L3FB'\n",
    "CLIENT_SECRET='UC52VVB1WLAD40QJIBUYHX54AMWFXPGJEMM4C0OZRGCOHWFZ'\n",
    "VERSION='20180608'\n",
    "LIMIT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import urllib.request # open and read URLs\n",
    "\n",
    "import json # handle JSON files\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "\n",
    "import requests # handle requests\n",
    "import pandas as pd # process data as dataframes with Pandas\n",
    "import numpy as np # handle data in a vectorized manner with NumPy\n",
    "\n",
    "# !conda install -c conda-forge geopy --yes # uncomment this line if you haven't installed the GeoPy geocoding library yet\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "\n",
    "##!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't installed the Folium library yet\n",
    "import folium # map rendering library\n",
    "\n",
    "# Matplolib plotting library and associated modules\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import random\n",
    "import geopy\n",
    "from geopy.distance import VincentyDistance\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn as sk  \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.tree import export_graphviz\n",
    "from subprocess import call\n",
    "from IPython.display import Image\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import svm\n",
    "from sklearn.cluster import KMeans # for K-Means clustering with Scikit-Learn\n",
    "\n",
    "print(\"Libraries imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eliminate_starbucks_venue_rows = True # It seems like we should eliminate actual Starbucks locations from our training data\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to gather recommended venues, with specifically the name and category, using the explore API\n",
    "def getNearbyVenues(latitudes, longitudes, radius=300, limit=100):\n",
    "    \n",
    "    count = 0\n",
    "    venues_list = []\n",
    "    test_list = []\n",
    "    for lat, lng in zip(latitudes, longitudes):\n",
    "        #create a unique name for this point that is easier to deal with than a combination of lat and lon\n",
    "        point_name = \"point\"+str(count).zfill(5)\n",
    "        if verbose:\n",
    "            print(point_name,':',lat,'-',lng)\n",
    "         \n",
    "        else:\n",
    "            # progress indicator for the impatient among us\n",
    "            if(count%100 == 0):\n",
    "                print(\"\\r{0}\".format(round((float(count)/len(latitudes))*100),2)+\"% done\")\n",
    "            \n",
    "        # create the API request URL\n",
    "        url = \"https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}\".format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            limit)\n",
    "            \n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"][\"groups\"][0][\"items\"]\n",
    "        \n",
    "        \n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            point_name,\n",
    "            lat, \n",
    "            lng, \n",
    "            v[\"venue\"][\"name\"], \n",
    "            v[\"venue\"][\"location\"][\"lat\"], \n",
    "            v[\"venue\"][\"location\"][\"lng\"],  \n",
    "            v[\"venue\"][\"location\"][\"distance\"], \n",
    "            v[\"venue\"][\"categories\"][0][\"name\"]) for v in results])        \n",
    "\n",
    "   \n",
    "        if (len(results)<1):\n",
    "            venues_list.append([(\n",
    "                point_name,\n",
    "                lat, \n",
    "                lng, \n",
    "                \"\", \n",
    "                0, \n",
    "                0,  \n",
    "                0, \n",
    "                \"No venues within radius\")])\n",
    "                \n",
    "                \n",
    "        test_list.append([(point_name) for v in results])\n",
    "        \n",
    "    \n",
    "        count = count + 1\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    print(\"nearby_venues length:\" + str(len(nearby_venues)))      \n",
    "    \n",
    "    nearby_venues.columns = [\"Point Name\",\n",
    "                  \"Neighborhood Latitude\", \n",
    "                  \"Neighborhood Longitude\", \n",
    "                  \"Venue\", \n",
    "                  \"Venue Latitude\", \n",
    "                  \"Venue Longitude\", \n",
    "                  \"Venue Distance\",           \n",
    "                  \"Venue Category\"]\n",
    "    print(\"# of unique points in nearby_venues =\"+str(len(nearby_venues[\"Point Name\"].unique())))\n",
    "    return(nearby_venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is possible that Foursqaure may have incorrect data\n",
    "# I have seen areas with venues where the API returned no results\n",
    "def getStarbucksLocations(lat, lng, limit=200):\n",
    "    \n",
    "    count = 0\n",
    "    starbucks_list = []\n",
    "    #for lat, lng in zip(latitudes, longitudes):\n",
    "        #create a unique name for this point that is easier to deal with than a combination of lat and lon\n",
    "    this_radius = 300\n",
    "         \n",
    "    \n",
    "            \n",
    "        # create the API request URL\n",
    "       # url = \"https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}\".format(\n",
    "        #url = \"https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&query={}\".format(\n",
    "    #url = \"https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&limit={}&ne={},{}&sw={},{}&query={}\".format(\n",
    "    url = \"https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&limit={}&radius={}&ll={},{}&query={}\".format(\n",
    "          \n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            limit,\n",
    "            this_radius,\n",
    "            lat,\n",
    "            lng,\n",
    "            \"Starbucks\")\n",
    "    sb_coords = []\n",
    "    # make the GET request\n",
    "   # results = requests.get(url).json()[\"response\"][\"venues\"][\"location\"]\n",
    "    results = requests.get(url).json()[\"response\"][\"venues\"]\n",
    "    for result in results:\n",
    "        sb_coords.append((result[\"location\"][\"lat\"],result[\"location\"][\"lng\"]))\n",
    "        \n",
    "    if (len(results)<1):\n",
    "        if verbose ==True:\n",
    "            print(\"no starbucks location results\")\n",
    "    return(sb_coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_lat=37.7749\n",
    "sf_lon=-122.4194"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llat=37.7\n",
    "ulat = 37.807957\n",
    "llon = -122.494329\n",
    "ulon = -122.395276\n",
    "\n",
    "points = []\n",
    "val_points = []\n",
    "radius = 300 #in meters\n",
    "current_lat = ulat\n",
    "current_lon = llon\n",
    "\n",
    "d = radius/1000 #in kilometers\n",
    "d=d*2\n",
    "b=90 #in degrees 90 = east,180=south,270=west\n",
    "# given: lat1, lon1, b = bearing in degrees, d = distance in kilometers      \n",
    " \n",
    "def lon_pass(current_lat, current_lon, ulat, ulon, pnts):\n",
    "    while (current_lat <= ulat) and (current_lon <= ulon) and (current_lat >= llat):    \n",
    "        origin = geopy.Point(current_lat, current_lon)\n",
    "        destination = VincentyDistance(kilometers=d).destination(origin, b)\n",
    "        current_lat, current_lon = destination.latitude, destination.longitude\n",
    "        if verbose==True:\n",
    "            print(current_lat, current_lon)\n",
    "        pnts.append(list((current_lat, current_lon)))\n",
    "    return(pnts)\n",
    "        \n",
    " \n",
    "        \n",
    "\n",
    "def generate_points(llat, llon, ulat, ulon):\n",
    "    pnts=[]\n",
    "    current_lat = ulat\n",
    "    current_lon = llon   \n",
    "    #while (current_lat <= ulat) and (current_lon <= ulon) and (current_lat >= llat) and (current_lon >= llon):\n",
    "    while (current_lat <= ulat) and (current_lon <= ulon) and (current_lat >= llat):\n",
    "        pnts = lon_pass(current_lat, current_lon, ulat, ulon, pnts)\n",
    "        print('new row of coordinates')  \n",
    "        current_lon=llon#move back to beginning of row   \n",
    "        origin = geopy.Point(current_lat, current_lon)\n",
    "        destination = VincentyDistance(kilometers=d).destination(origin, 180)#move south one step for next row\n",
    "        current_lat, current_lon = destination.latitude, destination.longitude       \n",
    "    return(pnts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = generate_points(llat, llon, ulat, ulon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We many need more examples of the near-Starbucks class for a balanced set - so get a bunch of points we know are near to Starbucks\n",
    "I pulled coordinates from the Kaggle Starbucks locations dataset<br />\n",
    "The precision of the Kaggle set wasn't good enough to ensure the Starbucks location would be within the radius chosen, so I found a different dataset with higher precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starbucks_heavy_coordinates = pd.read_csv('data/directory-us-ca.csv') #this file gave lat-lon tothe hundreths, but that isn't good enough to ensure a Starbucks is within my radius\n",
    "starbucks_heavy_coordinates = pd.read_csv('data/starbucks_us_ca_locations.csv',header=None)\n",
    "starbucks_heavy_coordinates.columns = [\"Longitude\", \"Latitude\", \"Location\", \"Address\"]\n",
    "# These coordinates aren't always precise enough to trigger a Starbucks location in the Foursquare API within the required radius\n",
    "# For that reason, pulling an equal number of coordinate points from this set doesn't necessarily balance the classes exactly\n",
    "# Sometimes two different coordinate points in this set are given for a single Starbucks location. They are rare enough to be ignored for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starbucks_heavy_coordinates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_needed_to_balance = int(len(points)*1.5) #This balances the classes better\n",
    "num_needed_to_balance = len(points) #This does a good enough job of balancing the classes\n",
    "num_needed_to_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shc_lat, shc_lon = starbucks_heavy_coordinates.Latitude, starbucks_heavy_coordinates.Longitude\n",
    "\n",
    "nnl=(shc_lat[0:num_needed_to_balance],shc_lon[0:num_needed_to_balance])\n",
    "pnnl = pd.DataFrame(nnl)\n",
    "pnnl.shape\n",
    "pnnl = pnnl.transpose()\n",
    "pnnl\n",
    "newlats=pnnl['Latitude']\n",
    "newlons=pnnl['Longitude']\n",
    "newlats\n",
    "points = pd.DataFrame(points)\n",
    "current_end = len(points)\n",
    "for i in range(0,num_needed_to_balance):\n",
    "    lenp = len(points)\n",
    "    points = points.set_value(lenp, 0, newlats[i])\n",
    "    points = points.set_value(lenp, 1, newlons[i])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuffle the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = points.sample(frac=1).reset_index(drop=True)\n",
    "points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_training_data = folium.Map(location=[ulat, ulon], zoom_start=11)\n",
    "\n",
    "\n",
    "# add markers to map\n",
    "for lat, lng in zip(points[0],points[1]):\n",
    "    if (lat <= ulat) and (lng*1.00005 <= ulon) and (lat >= llat) and (lng >=llon):#had to multiply by a tad to avoid rounding errors\n",
    "        marker_color = 'navy'\n",
    "    else:\n",
    "        marker_color = 'brown'\n",
    "        \n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=5,\n",
    "        color=marker_color,\n",
    "        #fill=True,\n",
    "        fill=False,\n",
    "        fill_color='#3186cc',\n",
    "        fill_opacity=0.7,\n",
    "        parse_html=False).add_to(map_training_data)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "map_training_data_legend_html =   '''\n",
    "                <div style=\"position: fixed; \n",
    "                            bottom: 50px; left: 44px; width: 148px; height: 130px; \n",
    "                            border:2px solid grey; z-index:9999; font-size:14px;\n",
    "                            #background: white;\n",
    "                            \">\n",
    "                              <div style=\"font-size:10px;\"><br />&nbsp; Training classes balanced<br /> \n",
    "                              &nbsp; with known Starbucks<br />\n",
    "                              &nbsp; locations throughout<br />\n",
    "                              &nbsp; California<br />  \n",
    "                              </div>\n",
    "                              \n",
    "                              &nbsp;&nbsp;&nbsp; unknown: &nbsp; <i class=\"fa fa-circle-o style=\"color:navy\"></i><br>\n",
    "                              &nbsp;&nbsp;&nbsp; known: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class=\"fa fa-circle-o\" style=\"color:brown\">\n",
    "                              </i>\n",
    "                </div>\n",
    "                ''' \n",
    "map_training_data.get_root().html.add_child(folium.Element(map_training_data_legend_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues = getNearbyVenues(latitudes = points[0],\n",
    "                              longitudes =  points[1]\n",
    "                               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues['Venue'].unique()\n",
    "venues['Venue'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues['Venue Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "onehot = pd.get_dummies(venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "# add neighborhood column back to dataframe\n",
    "onehot['point_name'] = venues['Point Name'] \n",
    "onehot['point_lat'] = venues['Neighborhood Latitude'] \n",
    "onehot['point_lon'] = venues['Neighborhood Longitude'] \n",
    "onehot['isStarbucks'] = (venues['Venue'] == \"Starbucks\")\n",
    "# move neighborhood column to the first column\n",
    "fixed_columns = [onehot.columns[-3]] + [onehot.columns[-2]] + [onehot.columns[-1]] + list(onehot.columns[:-3])\n",
    "onehot = onehot[fixed_columns]\n",
    "\n",
    "onehot.isStarbucks = onehot.isStarbucks.astype(int)\n",
    "\n",
    "onehot.head()\n",
    "onehot['isStarbucks'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_near_starbucks = onehot[onehot.isStarbucks > 0][\"point_name\"]\n",
    "if eliminate_starbucks_venue_rows == True:\n",
    "    #keep the isStarbucks flag but throw out other venue info\n",
    "    ##points_near_starbucks = onehot[onehot.isStarbucks > 0][\"point_name\"]\n",
    "    indexes_to_remember = onehot[onehot.isStarbucks > 0].index\n",
    "    points_near_starbucks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_near_starbucks_df = pd.DataFrame(points_near_starbucks)\n",
    "points_near_starbucks_df.head()\n",
    "\n",
    "for point_name in points_near_starbucks_df.point_name:\n",
    "    onehot.loc[onehot.point_name == point_name,\"isStarbucks\"] = 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if eliminate_starbucks_venue_rows == True:\n",
    "    # eliminate venues called Starbucks from the training data, as that unfairly \n",
    "    # improves prediction, even when de-indentified\n",
    "    # For example, the presence of a coffe shop at those coordinates\n",
    "    # makes for a powerfully predictive training feature\n",
    "    for ind in indexes_to_remember:\n",
    "        onehot = onehot.drop([ind], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shorter now if we removed Starbucks venues\n",
    "len(onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(onehot.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My intuition is that an area with more venues is more business-centric, <br />\n",
    "and that would affect the likelihood of a Starbucks being located nearby<br />\n",
    "Let's create a feature for the number of venues nearby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot['number_of_venues_within_radius'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(onehot.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the new feature 'number_of_venues_within_radius'\n",
    "# have to combine means and counts\n",
    "grouper = onehot.groupby('point_name').mean().reset_index()\n",
    "counter = onehot.groupby('point_name').count().reset_index()\n",
    "grouped = grouper\n",
    "grouped.iloc[:,-1] = counter.iloc[:,-1]\n",
    "max_of_venue_count = grouped.iloc[:,-1].max()\n",
    "# normalize the new feature\n",
    "grouped.iloc[:,-1] = grouped.iloc[:,-1]/max_of_venue_count\n",
    "grouped.iloc[:,-1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "num_top_venues = 5\n",
    "count = 0\n",
    "for point in grouped['point_name']:\n",
    "    temp = grouped[grouped['point_name'] == point].T.reset_index()\n",
    "    temp.columns = ['venue','freq']\n",
    "    temp = temp.iloc[2:]\n",
    "    temp['freq'] = temp['freq'].astype(float)\n",
    "    temp = temp.round({'freq': 2})\n",
    "    if verbose == True:\n",
    "        print(\"----\"+point+\"----\")\n",
    "        print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))\n",
    "        print('\\n')\n",
    "    else:\n",
    "        if count % 100 == 0:\n",
    "            print(\"----\"+point+\"----\")\n",
    "            print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))\n",
    "            print('\\n')\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = grouped['isStarbucks'] > 0\n",
    "y = pd.DataFrame(y)\n",
    "y = y.isStarbucks.astype(int)\n",
    "print(y.value_counts())\n",
    "X = grouped\n",
    "print(X.shape)\n",
    "X.drop(columns=['point_name', 'point_lat', 'point_lon', 'isStarbucks'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a random feature<br />\n",
    "Let's see how many of our features do no better than a randomly generated feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['random'] = np.random.random(size=len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.columns)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing vars\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdytrain=pd.DataFrame(y_train)\n",
    "pdytest=pd.DataFrame(y_test)\n",
    "pdytrain['isStarbucks'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdytest['isStarbucks'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLF:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def fit(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, name, kernel_type, X_train, y_train, X_test, y_test, X_val, y_val):\n",
    "        self.ClassifierType = \"Support Vector Machine\"\n",
    "        self.name = name \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val        \n",
    "        self.kernel_type = kernel_type\n",
    "        self.svm_model = svm.SVC(kernel=kernel_type, C=1, gamma=1, probability=True) \n",
    "        print(self.ClassifierType + \" - \" + name + \" created\")\n",
    "    def fit(self):\n",
    "        fit = self.svm_model.fit(X_train, y_train)\n",
    "    def predict(self, X):\n",
    "        predicted = self.svm_model.predict(X)\n",
    "    def score(self, X, y):\n",
    "        score = self.svm_model.score(X, y)\n",
    "    def print_scores(self):\n",
    "        self.svm_model.score(X_train,y_train)\n",
    "        print(self.name + \" Train accuracy = \" + str(self.svm_model.score(X_train,y_train)))\n",
    "        print(self.name + \" Test accuracy = \" + str(self.svm_model.score(X_test,y_test)))\n",
    "        print(self.name + \" Validation accuracy = \" + str(self.svm_model.score(X_val,y_val)))        \n",
    "        svm_prob_y_train = self.svm_model.predict_proba(X_train)\n",
    "        svm_prob_y_test = self.svm_model.predict_proba(X_test)      \n",
    "        svm_prob_y_val = self.svm_model.predict_proba(X_val)              \n",
    "        print(self.name + \" Train ROC AUC = \" + str(roc_auc_score(y_train, svm_prob_y_train[:,1])))\n",
    "        print(self.name + \" Test ROC AUC = \" + str(roc_auc_score(y_test, svm_prob_y_test[:,1]))) \n",
    "        print(self.name + \" Validation ROC AUC = \" + str(roc_auc_score(y_val, svm_prob_y_val[:,1])))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RF:\n",
    "    rfc = RandomForestClassifier(n_estimators=100, max_depth=15, random_state=0)\n",
    "    def __init__(self, name, X_train, y_train, X_test, y_test, X_val, y_val):\n",
    "        self.ClassifierType = \"Random Forest\"\n",
    "        self.name = name \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.y_train = X_test\n",
    "        self.y_train = y_test\n",
    "        self.y_train = X_val\n",
    "        self.y_train = y_val\n",
    "        \n",
    "        self.rfc = RandomForestClassifier(n_estimators=100, max_depth=15, random_state=0)\n",
    "        print(self.ClassifierType + \" - \" + name + \" created\")\n",
    "        \n",
    "    #def visualize_first_estimator(self):\n",
    "        \n",
    "    def get_estimator(self, estimator_num):        \n",
    "        estimator =  self.f.estimators_[estimator_num]\n",
    "        return(estimator)\n",
    "        \n",
    "    def fit(self):\n",
    "        self.f = self.rfc.fit(X_train,y_train)\n",
    "    def predict(self, X):\n",
    "        self.predictions = RF.rfc.predict(X)   \n",
    "        #print(self.predictions)\n",
    "    def predict_all(self, X):\n",
    "        self.test_predictions = RF.rfc.predict(X_test)   \n",
    "        self.val_predictions = RF.rfc.predict(X_val)           \n",
    "        #print(self.predictions)              \n",
    "        \n",
    "    def get_and_print_results(self):\n",
    "        self.prob_y_train = self.rfc.predict_proba(X_train)\n",
    "        self.prob_y_test = self.rfc.predict_proba(X_test)\n",
    "        if len(X_val) > 0 and len(y_val) > 0 :\n",
    "            self.prob_y_val = self.rfc.predict_proba(X_val)\n",
    "        \n",
    "        self.num_features = self.f.n_features_\n",
    "        print(self.num_features)        \n",
    "        self.train_accuracy = self.rfc.score(X_train,y_train)\n",
    "        self.test_accuracy = self.rfc.score(X_test,y_test)\n",
    "        if len(X_val) > 0 and len(y_val) > 0 :\n",
    "            self.validation_accuracy = self.rfc.score(X_val,y_val)        \n",
    "        \n",
    "        self.feature_importances = self.rfc.feature_importances_\n",
    "        print(self.name + \" Train accuracy = \" + str(self.train_accuracy))  \n",
    "        print(self.name + \" Train ROC AUC = \" + str(roc_auc_score(y_train, self.prob_y_train[:,1])))\n",
    "        print(self.name + \" Test accuracy = \" + str(self.test_accuracy))  \n",
    "        print(self.name + \" Test ROC AUC = \" + str(roc_auc_score(y_test, self.prob_y_test[:,1])))\n",
    "        \n",
    "        print(self.name + \" Validation accuracy = \" + str(self.validation_accuracy))  \n",
    "        print(self.name + \" Validation ROC AUC = \" + str(roc_auc_score(y_val, self.prob_y_val[:,1])))\n",
    "        print(self.name + \" Num Features: \" + str(self.num_features))  \n",
    "       \n",
    "        self.feature_importance_info = []\n",
    "        for i in range(0,self.num_features):\n",
    "            if self.feature_importances[i] >= 0:\n",
    "                self.feature_importance_info.append((self.feature_importances[i],X_train.columns[i]))\n",
    "    \n",
    "        self.feature_importance_df = pd.DataFrame(self.feature_importance_info)\n",
    "        self.feature_importance_df.rename(columns={0: \"Feature Importance\", 1: \"Feature\"}, inplace=True)\n",
    "        self.feature_importance_df.sort_values(by=\"Feature Importance\",ascending=False,inplace=True)    \n",
    "        \n",
    "        self.feature_importance_df.set_index('Feature',inplace=True)\n",
    "        self.feature_importance_df       \n",
    "\n",
    "        \n",
    "        \n",
    "    def get_and_print_results_without_validation(self):\n",
    "        self.prob_y_train = self.rfc.predict_proba(X_train)\n",
    "        self.prob_y_test = self.rfc.predict_proba(X_test)\n",
    "        #if len X_val >0 and len y_val > 0 :\n",
    "        #    self.prob_y_val = self.rfc.predict_proba(X_val)\n",
    "        \n",
    "        self.num_features = self.f.n_features_\n",
    "        print(self.num_features)        \n",
    "        self.train_accuracy = self.rfc.score(X_train,y_train)\n",
    "        self.test_accuracy = self.rfc.score(X_test,y_test)\n",
    "        \n",
    "        self.feature_importances = self.rfc.feature_importances_\n",
    "        print(self.name + \" Train accuracy = \" + str(self.train_accuracy))  \n",
    "        print(self.name + \" Train ROC AUC = \" + str(roc_auc_score(y_train, self.prob_y_train[:,1])))\n",
    "        print(self.name + \" Test accuracy = \" + str(self.test_accuracy))  \n",
    "        print(self.name + \" Test ROC AUC = \" + str(roc_auc_score(y_test, self.prob_y_test[:,1])))\n",
    "        \n",
    "        print(self.name + \" Validation accuracy = \" + str(self.validation_accuracy))  \n",
    "        print(self.name + \" Validation ROC AUC = \" + str(roc_auc_score(y_val, self.prob_y_val[:,1])))\n",
    "        \n",
    "        \n",
    "        print(self.name + \" Num Features: \" + str(self.num_features))  \n",
    "       \n",
    "        self.feature_importance_info = []\n",
    "        for i in range(0,self.num_features):\n",
    "            if self.feature_importances[i] >= 0:\n",
    "                self.feature_importance_info.append((self.feature_importances[i],X_train.columns[i]))\n",
    "    \n",
    "        self.feature_importance_df = pd.DataFrame(self.feature_importance_info)\n",
    "        self.feature_importance_df.rename(columns={0: \"Feature Importance\", 1: \"Feature\"}, inplace=True)\n",
    "        self.feature_importance_df.sort_values(by=\"Feature Importance\",ascending=False,inplace=True)    \n",
    "        \n",
    "        self.feature_importance_df.set_index('Feature',inplace=True)\n",
    "        self.feature_importance_df       \n",
    "        \n",
    "        \n",
    "        \n",
    "print(\"refreshed\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### validate #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starbucks heavy area in Illinois\n",
    "#41.916715, -87.989170\n",
    "#41.843114, -87.779305\n",
    "validate_llat = 41.843114\n",
    "validate_llon = -87.989170\n",
    "validate_ulat = 41.916715\n",
    "validate_ulon = -87.779305\n",
    "val_points = generate_points(validate_llat, validate_llon, validate_ulat, validate_ulon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_points=pd.DataFrame(val_points)\n",
    "len(validation_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_points = validation_points.sample(frac=1).reset_index(drop=True)\n",
    "validation_points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(validation_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_venues = getNearbyVenues(latitudes = validation_points[0],\n",
    "                              longitudes =  validation_points[1]\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate completely new data to try "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(validation_venues['Point Name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_venues.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_venues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nelat = 41.962643\n",
    "#nelng = -87.757414\n",
    "#swlat = 41.903702\n",
    "#swlng = -87.896807\n",
    "nelat = validate_llat\n",
    "nelng = validate_llon\n",
    "swlat = validate_ulat\n",
    "swlng = validate_ulon\n",
    "sb_locations = []\n",
    "#for lat, lng in zip(validation_answer_key['point_lat'],validation_answer_key['point_lon']):   \n",
    "count=0\n",
    "for lat, lng in zip(validation_points[0],validation_points[1]):    \n",
    "    sb_location_res = getStarbucksLocations(lat, lng, limit=200)\n",
    "    for sb_location in sb_location_res:\n",
    "        print(\"lat=\" + str(sb_location[0]) + \"lon=\" + str(sb_location[1])) \n",
    "        sb_locations.append((sb_location[0],sb_location[1]))   \n",
    "        #print(sb_location)\n",
    "    if(count%20 == 0):\n",
    "                print(\"\\r{0}\".format(round((float(count)/len(validation_points[0]))*100),2)+\"% done\")\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_venues.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_locations_df = pd.DataFrame(sb_locations)\n",
    "sb_locations_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_venues['Venue'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "validation_onehot = pd.get_dummies(validation_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add neighborhood column back to dataframe\n",
    "validation_onehot['point_name'] = validation_venues['Point Name'] \n",
    "validation_onehot['point_lat'] = validation_venues['Neighborhood Latitude'] \n",
    "validation_onehot['point_lon'] = validation_venues['Neighborhood Longitude'] \n",
    "validation_onehot['isStarbucks'] = (validation_venues['Venue'] == \"Starbucks\")# should do a better fuzzy match\n",
    "# move neighborhood column to the first column\n",
    "#validation_fixed_columns = [validation_onehot.columns[-3]] + [validation_onehot.columns[-2]] + [validation_onehot.columns[-1]] + list(validation_onehot.columns[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move neighborhood column to the first column\n",
    "validation_fixed_columns = [validation_onehot.columns[-3]] + [validation_onehot.columns[-2]] + [validation_onehot.columns[-1]] + list(validation_onehot.columns[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(validation_fixed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_onehot.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_fixed_columns = [validation_onehot.columns[-3]] + [validation_onehot.columns[-2]] + [validation_onehot.columns[-1]] + list(validation_onehot.columns[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_onehot = validation_onehot[validation_fixed_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_onehot.isStarbucks = validation_onehot.isStarbucks.astype(int)\n",
    "validation_onehot.head()\n",
    "validation_onehot['isStarbucks'].value_counts()\n",
    "validation_venues.head()\n",
    "len(validation_onehot)\n",
    "validation_onehot.head()\n",
    "validation_onehot.shape\n",
    "validation_answer_key = validation_onehot\n",
    "validation_answer_key.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_answer_key = onehot\n",
    "test_answer_key.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_answer_key = validation_answer_key.groupby('point_name').mean()\n",
    "validation_answer_key.head()\n",
    "validation_answer_key.shape\n",
    "validation_answer_key = validation_onehot.groupby('point_name').mean().reset_index()\n",
    "validation_answer_key.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_points_near_starbucks = validation_onehot[validation_onehot.isStarbucks > 0][\"point_name\"]\n",
    "if eliminate_starbucks_venue_rows == True:\n",
    "    #keep the isStarbucks flag but throw out other venue info\n",
    "    ##points_near_starbucks = onehot[onehot.isStarbucks > 0][\"point_name\"]\n",
    "    validation_indexes_to_remember = validation_onehot[validation_onehot.isStarbucks > 0].index\n",
    "    validation_points_near_starbucks\n",
    "validation_points_near_starbucks_df = pd.DataFrame(validation_points_near_starbucks)\n",
    "validation_points_near_starbucks_df.head()\n",
    "for point_name in validation_points_near_starbucks_df.point_name:\n",
    "    validation_onehot.loc[validation_onehot.point_name == point_name,\"isStarbucks\"] = 1\n",
    "len(validation_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " if eliminate_starbucks_venue_rows == True:\n",
    "    for ind in validation_indexes_to_remember:\n",
    "        validation_onehot = validation_onehot.drop([ind], axis=0)\n",
    "len(validation_onehot)\n",
    "\n",
    "len(validation_onehot.columns)\n",
    "validation_onehot['number_of_venues_within_radius'] = 1\n",
    "len(validation_onehot.columns)\n",
    "validation_onehot['point_name']       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the new feature 'number_of_venues_within_radius'\n",
    "# have to combine means and counts\n",
    "validation_grouper = validation_onehot.groupby('point_name').mean().reset_index()\n",
    "validation_counter = validation_onehot.groupby('point_name').count().reset_index()\n",
    "validation_grouped = validation_grouper\n",
    "validation_grouped.iloc[:,-1] = validation_counter.iloc[:,-1]\n",
    "validation_max_of_venue_count = validation_grouped.iloc[:,-1].max()\n",
    "# normalize the new feature\n",
    "validation_grouped.iloc[:,-1] = validation_grouped.iloc[:,-1]/validation_max_of_venue_count\n",
    "validation_grouped.iloc[:,-1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "num_top_venues = 5\n",
    "count = 0\n",
    "for point in validation_grouped['point_name']:\n",
    "    validation_temp = validation_grouped[validation_grouped['point_name'] == point].T.reset_index()\n",
    "    validation_temp.columns = ['venue','freq']\n",
    "    validation_temp = validation_temp.iloc[2:]\n",
    "    validation_temp['freq'] = validation_temp['freq'].astype(float)\n",
    "    validation_temp = validation_temp.round({'freq': 2})\n",
    "    if verbose == True:\n",
    "        print(str(count)+\"----\"+str(point)+\"----\")\n",
    "        print(validation_temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))\n",
    "        print('\\n')\n",
    "    else:\n",
    "        if count % 100 == 0:\n",
    "            print(\"----\"+point+\"----\")\n",
    "            print(validation_temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))\n",
    "            print('\\n')\n",
    "    count = count + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_grouped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_grouped.head()\n",
    "validation_y = validation_grouped['isStarbucks'] > 0\n",
    "validation_y = pd.DataFrame(validation_y)\n",
    "validation_y = validation_y.isStarbucks.astype(int)\n",
    "print(validation_y.value_counts())\n",
    "validation_X = validation_grouped\n",
    "print(validation_X.shape)\n",
    "validation_map_data = validation_X\n",
    "validation_X.drop(columns=['point_name', 'point_lat', 'point_lon', 'isStarbucks'], inplace=True)\n",
    "validation_X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_X['random'] = np.random.random(size=len(validation_X))\n",
    "print(validation_X.columns)\n",
    "print(validation_X.shape)\n",
    "validation_pdy=pd.DataFrame(validation_y)\n",
    "validation_pdy['isStarbucks'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yikes - my features don't match\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see if we can fit this square peg into that round hole\n",
    "matchup_validation_X = []\n",
    "matchup_validation_X = pd.DataFrame(matchup_validation_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchup_validation_X = pd.DataFrame(0, index=np.arange(len(validation_X)), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val_col in validation_X.columns:\n",
    "    matchup_validation_X[val_col] = validation_X[val_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchup_validation_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have some extra columns at the end\n",
    "Would be nice to have incorporated those extra rows at the beginning\n",
    "For now, we just throw those extra columns out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(matchup_validation_X.columns) > len(X.columns):\n",
    "    matchup_validation_X.drop(matchup_validation_X.columns[len(matchup_validation_X.columns)-1], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchup_validation_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_X = matchup_validation_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val=validation_X\n",
    "y_val=validation_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "##############\n",
    "#### validate end ###\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = X.corr()\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features are predominantly uncorrelated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = X.corr().abs()\n",
    "s = c.unstack()\n",
    "so = s.sort_values(kind=\"quicksort\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_correlations = []\n",
    "correlation_threshold = 0.7\n",
    "for colname in c.columns:\n",
    "    if verbose == True:\n",
    "        print(\"check for features strongly coorelated with \" + colname)\n",
    "    for i in range(0, len(c)):\n",
    "        this_correlation = c[colname][i]\n",
    "        if this_correlation >= correlation_threshold:\n",
    "            if colname != c.index[i]: #don't act on self correlations\n",
    "                if verbose == True:\n",
    "                    print(\"   \" + c.index[i] + \" - \" + str(this_correlation))\n",
    "                strong_correlations.append((colname,c.index[i],this_correlation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether or not I do anything about correlated features, they are interesting to look at.<br />\n",
    "Some of them appear to be negatively correlated even I only show the magnitude of the correlation here.<br />\n",
    "Some of the correlations look like they are due to small samples, such as the Roller Rink in San Francisco that is perfectly correlated with a Spiritual Center. I've actually skated there.<br />\n",
    "Some correlations look meaningful, such as the correlation between aquariums and zoos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_correlations_df = pd.DataFrame(strong_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_correlations_df[\"feature_pair\"]=strong_correlations_df[0]+\"-\"+strong_correlations_df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_correlations_df.drop([0,1],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_correlations_df.rename(columns= {2: \"correlaton\"}, inplace=True)\n",
    "strong_correlations_df.sort_values(by=\"correlaton\",ascending=False,inplace=True)\n",
    "strong_correlations_df.set_index(\"feature_pair\", drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to see if there were strong correlations in the features, so I could remove some.<br />\n",
    "With 400 or so features and something like 600 data points, I thought it would be good to remove redundant features.<br />\n",
    "I found strongly correlated features, but my performance was good.<br /> I learned that Random Forests can be\n",
    "tolerant of correlated features, so removing them seems uneccesary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strong_correlations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_all_features_1 = RF(\"RF_all_features_1\", X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "RF_all_features_1.fit()\n",
    "RF_all_features_1.get_and_print_results()\n",
    "RF_all_features_1.feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the test accuracy is a lot lower than the train accuracy, it could be a sign of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC Area Under Curve is a good way of measuring how well the binary classifier separates the two classes, even if the classes are unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_all_features_1.rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator =  RF_all_features_1.get_estimator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as dot file\n",
    "export_graphviz(estimator, out_file='tree.dot', \n",
    "                feature_names = X.columns,\n",
    "                class_names = ('near-Starbucks','not-near'),\n",
    "                rounded = True, proportion = True, \n",
    "                precision = 2, filled = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a tree<br />\n",
    "Note: this is only one tree <br />\n",
    "Random Forest takes a vote from many trees <br />\n",
    "It is possible however, to use only one tree (with some loss of accuracy),<br />\n",
    "and interpret how the classifier is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to png using system command (requires Graphviz)\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "# Display in jupyter notebook\n",
    "Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we do better using recursive feature elimination?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_limited_features = RandomForestClassifier(n_estimators=100, max_depth=15, random_state=0)  \n",
    "rfecv = RFECV(estimator=rf_limited_features, step=1, cv=2, scoring='roc_auc', verbose=2) \n",
    "selector=rfecv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sk.metrics.SCORERS.keys()#this shows the scoring metric choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_features_preds = selector.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_score = selector.score(X_test,y_test)\n",
    "selector_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.n_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selector picked 140 features, but that looks like a noisy spike<br />\n",
    "Half that (~70) looks like it would do fine, but there is no real loss by keeping extra features<br />\n",
    "Random Forest model does fairly well with extra features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give svm a try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_rbf = SVM(\"SVM (with rbf kernel)\",'rbf', X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "SVM_rbf.fit()\n",
    "SVM_rbf.predict(X_test)\n",
    "SVM_rbf.print_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_linear = SVM(\"SVM (with linear kernel)\",'linear', X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "SVM_linear.fit()\n",
    "SVM_linear.predict(X_test)\n",
    "SVM_linear.print_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_poly = SVM(\"SVM (with poly kernel)\",'poly', X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "SVM_poly.fit()\n",
    "SVM_poly.predict(X_test)\n",
    "SVM_poly.print_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_sigmoid = SVM(\"SVM (with sigmoid kernel)\",'sigmoid', X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "SVM_sigmoid.fit()\n",
    "SVM_sigmoid.predict(X_test)\n",
    "SVM_sigmoid.print_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like RandomForest and SVM with an rbf kernel do well<br />\n",
    "Maybe we should try those with smaller feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tried dropping my added feature but lets keep it\n",
    "#X_train.drop(['number_of_venues_within_radius'], axis=1)\n",
    "#X_test.drop(['number_of_venues_within_radius'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF_2 = RF(\"Random Forest #2\", X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "#RF_2.fit()\n",
    "#RF_2.get_and_print_results()\n",
    "#RF_2.feature_importance_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_testing_data = folium.Map(location=[ulat, ulon], zoom_start=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_answer_key_unique = pd.DataFrame(test_answer_key.groupby(['point_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_answer_key_unique = test_answer_key[['point_name','point_lat','point_lon','isStarbucks']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_point_name = ''\n",
    "# add markers for my predicted classes to map\n",
    "#for lat, lng in zip(validation_venues['Neighborhood Latitude'],validation_venues['Neighborhood Longitude']):\n",
    "##for lat, lng in zip(validation_points[0],validation_points[1]):\n",
    "count = 0\n",
    "for point_name, lat, lng, isSB in zip(test_answer_key['point_name'],test_answer_key['point_lat'],test_answer_key['point_lon'],test_answer_key['isStarbucks']):    \n",
    "\n",
    "    \n",
    "    if point_name != last_point_name:\n",
    "        \n",
    "        if isSB > 0 :\n",
    "            color_str = 'green'\n",
    "            #print(color_str)\n",
    "        else:\n",
    "            color_str = 'blue'\n",
    "            #print(color_str)\n",
    "  \n",
    "        if count < 2000:\n",
    "            folium.CircleMarker(\n",
    "                [lat, lng],\n",
    "                radius=5,\n",
    "                color=color_str,        \n",
    "                fill=False,\n",
    "                fill_color='#3186cc',\n",
    "                fill_opacity=0.7,\n",
    "                parse_html=False).add_to(map_testing_data)  \n",
    "        count = count + 1\n",
    "    #print(point_name)     \n",
    "        sb_test_locs = getStarbucksLocations(lat, lng, limit=200)\n",
    "        sb_test_locs_df = pd.DataFrame(sb_test_locs)\n",
    "        sb_test_locs_df.head()\n",
    "\n",
    "        if len(sb_test_locs) > 0: \n",
    "            for sblat, sblng in zip(sb_test_locs_df[0],sb_test_locs_df[1]):                  \n",
    "                folium.CircleMarker(\n",
    "                    [sblat, sblng],\n",
    "                    radius=5,\n",
    "                    color='red',        \n",
    "                    fill=False,\n",
    "                    fill_opacity=0.7,\n",
    "                    parse_html=False).add_to(map_testing_data)  \n",
    "        count = count + 1\n",
    "    last_point_name = point_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_testing_data_legend_html =   '''\n",
    "                <div style=\"position: fixed; \n",
    "                            bottom: 50px; left: 500px; width: 168px; height: 130px; \n",
    "                            border:2px solid grey; z-index:9999; font-size:14px;\n",
    "                            #background: white;\n",
    "                            \">\n",
    "                              <div style=\"font-size:10px;\"><br />&nbsp; Class 1 is predicted to be within <br /> \n",
    "                              &nbsp; 300m of a Starbucks location <br />\n",
    "                              &nbsp;<br />\n",
    "                             \n",
    "                              </div>\n",
    "                         \n",
    "                              &nbsp;&nbsp;&nbsp; predicted class 0: &nbsp; <i class=\"fa fa-circle-o style=\"color:blue\"></i><br>\n",
    "                              &nbsp;&nbsp;&nbsp; predicted class 1: &nbsp; <i class=\"fa fa-circle-o style=\"color:green\"></i><br>                              \n",
    "                              &nbsp;&nbsp;&nbsp; true class 1: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class=\"fa fa-circle-o\" style=\"color:red\">\n",
    "                              </i>\n",
    "                </div>\n",
    "                ''' \n",
    "map_testing_data.get_root().html.add_child(folium.Element(map_testing_data_legend_html))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_center_lat = validate_llat+(validate_ulat-validate_llat)/2\n",
    "validate_center_lon = validate_llon+(validate_ulon-validate_llon)/2\n",
    "validation_map = folium.Map(location=[validate_center_lat, validate_center_lon], zoom_start=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add markers for known Starbucks to map\n",
    "for lat, lng in zip(sb_locations_df[0],sb_locations_df[1]):    \n",
    "    color_str = 'red'\n",
    "\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=5,\n",
    "        color=color_str,        \n",
    "        fill=False,\n",
    "        #fill_color='#3186cc',\n",
    "        fill_color='#3186cc',\n",
    "        fill_opacity=0.7,\n",
    "        parse_html=False).add_to(validation_map)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "map_validation_data_legend_html =   '''\n",
    "                <div style=\"position: fixed; \n",
    "                            bottom: 50px; left: 30px; width: 168px; height: 130px; \n",
    "                            border:2px solid grey; z-index:9999; font-size:14px;\n",
    "                            #background: white;\n",
    "                            \">\n",
    "                              <div style=\"font-size:10px;\"><br />&nbsp; Class 1 is predicted<br /> \n",
    "                              &nbsp; to be within 300m of <br />\n",
    "                              &nbsp; aStarbucks location<br />\n",
    "                             \n",
    "                              </div>\n",
    "                              \n",
    "                              &nbsp;&nbsp;&nbsp; predicted class 0: &nbsp; <i class=\"fa fa-circle-o style=\"color:blue\"></i><br>\n",
    "                              &nbsp;&nbsp;&nbsp; predicted class 1: &nbsp; <i class=\"fa fa-circle-o style=\"color:green\"></i><br>                              \n",
    "                              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; true class 1: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class=\"fa fa-circle-o\" style=\"color:red\">\n",
    "                              </i>\n",
    "                </div>\n",
    "                ''' \n",
    "validation_map.get_root().html.add_child(folium.Element(map_validation_data_legend_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add markers for my predicted classes to map\n",
    "#for lat, lng in zip(validation_venues['Neighborhood Latitude'],validation_venues['Neighborhood Longitude']):\n",
    "##for lat, lng in zip(validation_points[0],validation_points[1]):\n",
    "for lat, lng, isSB in zip(validation_answer_key['point_lat'],validation_answer_key['point_lon'],validation_answer_key['isStarbucks']):    \n",
    "\n",
    "    if isSB > 0 :\n",
    "        color_str = 'green'\n",
    "    else:\n",
    "        color_str = 'blue'\n",
    "\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=5,\n",
    "        color=color_str,        \n",
    "        fill=False,\n",
    "        fill_color='#3186cc',\n",
    "        fill_opacity=0.7,\n",
    "        parse_html=False).add_to(validation_map)  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### end validate #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_importance_df = RF_2.feature_importance_df\n",
    "feature_importance_df = RF_all_features_1.feature_importance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save original matrices before you start cutting them up (just for safe keeping in case needed again)\n",
    "X_train_orig = X_train.copy(deep=True)\n",
    "y_train_orig = y_train.copy(deep=True)\n",
    "X_test_orig = X_test.copy(deep=True)\n",
    "y_test_orig = y_test.copy(deep=True)\n",
    "X_val_orig = X_val.copy(deep=True)\n",
    "y_val_orig = y_val.copy(deep=True)\n",
    "feature_importance_df_orig = feature_importance_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "\n",
    "def drop_features_below_cutoff(cutoff, feature_importance_df, this_df):\n",
    "    count = 0\n",
    "    num_to_do = feature_importance_df.size - cutoff\n",
    "    for feature in feature_importance_df[cutoff:feature_importance_df.size].index: \n",
    "        if verbose == True:\n",
    "            print(\"removing feature: \" + str(feature) + \" with importance = \" + str(feature_importance_df.loc[feature]['Feature Importance']))\n",
    "        try:\n",
    "            this_df.drop(columns=[feature], inplace=True)\n",
    "        except:\n",
    "            if verbose == True:\n",
    "                print(\"EXCEPTION - removing feature: \" + str(feature) + \" with importance = \" + str(feature_importance_df.loc[feature]['Feature Importance']))\n",
    "        \n",
    "        \n",
    "        # progress indicator for the impatient among us\n",
    "        if(count%20 == 0):\n",
    "            pass\n",
    "           # print(\"\\r{0}\".format(round((float(count)/num_to_do*100),2)+\"% done\"))\n",
    "           # print(\"\\r{0}\".format(round((float(count/num_to_do*100)),2)+\"% done\"))            \n",
    "            \n",
    "        count = count + 1            \n",
    "    return(this_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "def drop_features_on_all_sets(cutoff, feature_importance_df, X_train, X_test, X_val):\n",
    "    X_train = drop_features_below_cutoff(cutoff, feature_importance_df, X_train)\n",
    "    X_test = drop_features_below_cutoff(cutoff, feature_importance_df, X_test)\n",
    "    X_val = drop_features_below_cutoff(cutoff, feature_importance_df, X_val) \n",
    "    return(X_train, X_test, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train.columns)\n",
    "X_train = X_train_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train_orig\n",
    "#X_test = X_test_orig\n",
    "#X_val = X_val_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_orig.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df.iloc[112].name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features with importance below cutoff\n",
    "cutoff = selector.n_features_\n",
    "X_train, X_test, X_val  = drop_features_on_all_sets(cutoff, feature_importance_df, X_train, X_test, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = X_train_orig\n",
    "print(X_train_orig.columns)\n",
    "print(X_test_orig.columns)\n",
    "print(X_val_orig.columns)\n",
    "\n",
    "X_train.columns[100:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_3 = RF(\"Random Forest #3\", X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "RF_3.fit()\n",
    "RF_3.get_and_print_results()\n",
    "RF_3.feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 50\n",
    "# drop features with importance below cutoff\n",
    "X_train, X_test, X_val  = drop_features_on_all_sets(cutoff, feature_importance_df, X_train, X_test, X_val)\n",
    "\n",
    "#X_train = drop_features_below_cutoff(cutoff, feature_importance_df, X_train)\n",
    "#X_test = drop_features_below_cutoff(cutoff, feature_importance_df, X_test)\n",
    "#X_val = drop_features_below_cutoff(cutoff, feature_importance_df, X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_4 = RF(\"Random Forest #4\", X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "RF_4.fit()\n",
    "RF_4.get_and_print_results()\n",
    "RF_4.feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now let's set the cutoff to the feature importance level where the selector plot first hits<br />\n",
    "the high plateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 20\n",
    "# drop features with importance below cutoff\n",
    "X_train, X_test, X_val  = drop_features_on_all_sets(cutoff, feature_importance_df, X_train, X_test, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_orig.columns)\n",
    "print(X_test_orig.columns)\n",
    "print(X_val_orig.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features with importance below cutoff\n",
    "#cutoff = 20\n",
    "#X_train = drop_features_below_cutoff(cutoff, feature_importance_df, X_train)\n",
    "#X_test = drop_features_below_cutoff(cutoff, feature_importance_df, X_test)   \n",
    "#X_val = drop_features_below_cutoff(cutoff, feature_importance_df, X_val)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_5 = RF(\"Random Forest #5\", X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "RF_5.fit()\n",
    "RF_5.get_and_print_results()\n",
    "RF_5.feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the position of the random feature in the sorted feature_importances_df\n",
    "random_position = len(feature_importance_df) # default to max\n",
    "for i in range(0, len(feature_importance_df)):\n",
    "    #print(feature_importance_df.iloc[i].name)\n",
    "    if feature_importance_df.iloc[i].name == 'random':\n",
    "        random_position = i\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_position\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set the cutoff to the feature importance level of the random feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features with importance below cutoff\n",
    "cutoff = random_position\n",
    "X_train = drop_features_below_cutoff(cutoff, feature_importance_df, X_train)\n",
    "X_test = drop_features_below_cutoff(cutoff, feature_importance_df, X_test)    \n",
    "X_val = drop_features_below_cutoff(cutoff, feature_importance_df, X_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_6 = RF(\"Random Forest #6\", X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "RF_6.fit()\n",
    "RF_6.get_and_print_results()\n",
    "RF_6.feature_importance_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
